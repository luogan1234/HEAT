# HEAT

This project provides an attention-based neural network model for the entity-level typing task. It processes heterogeneous data and learns representations from them, including name, related paragraphs, and image features.

While **Mention-level Entity Typing** infers the types of a mention that are supported by its textual contexts, **Entity-level Typing** infers types of an entity by considering all types supported by data.

### Requirements and initialization

- PyTorch version >= 1.7
- Python version >= 3.7
- transformers >= 3.3
- a GPU with 11GB graphic RAM if run with the BiLSTM encoder
- a GPU with 32GB graphic RAM if run with the BERT encoder

Run `python init.py` to download datasets. Run `bash run.sh` to train the model with each dataset.

### Usage

- `-task` specifies the task id

- `-dataset` specifies the dataset name
- `-text_encoder` specifies the text encoder in {'bert', 'bert_freeze', 'lstm'}
- `-remove_name, -remove_para, -remove_img` make the model not use corresponding modules
- `-without_token_attention, -without_cross_modal_attention` make the model not use corresponding attention layers
- `-seed` specifies the random seed id
- `-consistency` makes the model do consistency training together
- `-labeled_num` limits the labeled samples number
- `-cpu` runs on cpu
- Other hyper-parameters are set in `config.py`

### Dataset

- `data.pkl` contains the name, raw paragraphs, and each typing task's labels for an entity
- `data_txt.pkl` is generated by `data_loader.py` and contains the tokenized indexes of the name and paragraphs
- `data_img.pkl` (optional) contains each entity's relevant image features
- `split.pkl` (optional) contains the split information of train/valid/test sets by entity's name
- `types.json` contains the name of each type
- `hierarchies.json` contains the taxonomy between types